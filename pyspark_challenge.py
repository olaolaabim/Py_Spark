# -*- coding: utf-8 -*-
"""Pyspark_challenge

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KH1EWT8f4vH57P_2mnhXlSr6yo1-XCOO
"""

pip install pyspark

pip install pyspark

from pyspark import SparkConf

from pyspark.context import SparkContext
conf = SparkConf().setAppName('Pyspark').setMaster('local')
sc = SparkContext(conf=conf)

"""sample"""

values = [1,2,3,4,5]
rdd = sc.parallelize(values)

rdd.take(5)

from google.colab import files
uploaded = files.upload()



rdd = sc.textFile("Pyspark.txt")

rdd.take(1)

rdd.collect()

aba = sc.parallelize(range(1,10000,2))
aba.persist()

textfile = sc.textFile("Pyspark.txt")
textfile.cache()

"""Map"""

x = sc.parallelize(["spark", "rdd", "example", "sample", "example"])
y = x.map(lambda x:(x,1))
y.collect()

"""FlatMap"""

rdd = sc.parallelize([2,3,4])
sorted(rdd.flatMap(lambda x: range(1,x)).collect())

"""Filter"""

rdd = sc.parallelize([1,2,3,4,5])
rdd.filter(lambda x: x%2 == 0).collect()

"""Sample"""

parrallel = sc.parallelize(range(9))
parrallel.sample(True, .2).count()
parrallel.sample(False,1).collect()

"""union"""

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.union(par).collect()

"""intersection"""

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.intersection(par).collect()

"""distinct"""

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.union(par).distinct().collect()

"""sortby"""

y = sc.parallelize([5,7,1,3,2,1])
y.sortBy(lambda c: c, True).collect()

"""sortby"""

z = sc.parallelize([("H", 10), ("A", 26),("F", 3) ,("Z", 1), ("L", 5)])
z.sortBy(lambda c: c, False).collect()

"""MapPartitions"""

rdd = sc.parallelize([1,2,3,4], 2)
def f(iterator): yield sum(iterator)
rdd.mapPartitions(f).collect()

"""MapPartitions - Withindex"""

rdd = sc.parallelize([1,2,3,4], 4)
def f(splitIndex, Iterator): yield splitIndex
rdd.mapPartitionsWithIndex(f).sum()

"""GroupBy"""

rdd = sc.parallelize([1,1,2,3,5,8])
result = rdd.groupBy(lambda x: x%2).collect()
sorted ([(x, sorted(y)) for (x,y) in result])

"""keyBy"""

x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)
y = sc.parallelize(zip(range(0,5), range(0,5)))
[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]

"""Zip"""

x = sc.parallelize(range(0,5))
y = sc.parallelize(range(1000, 1005))
x.zip(y).collect()

"""Zip - Withinindex"""

sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()

"""repartition"""

rdd = sc.parallelize([1,2,3,4,5,6,7], 4)
sorted(rdd.glom().collect())
len(rdd.repartition(2).glom().collect())

"""coalesce"""

sc.parallelize([1,2,3,4,5], 3).glom().collect()
sc.parallelize([1,2,3,4,5], 3).coalesce(2).glom().collect()

"""Reduce"""

from operator import add
sc.parallelize([1,2,3,4,5]).reduce(add)
sc.parallelize((2 for _ in range(10))).map(lambda x:1).cache().reduce(add)

"""first"""

sc.parallelize([2,3,4]).first()

"""takeOrdered"""

nums = sc.parallelize([1,5,3,9,4,0,2])
nums.takeOrdered(5)

"""Take"""

nums = sc.parallelize([1,5,3,9,4,0,2])
nums.take(5)

"""Count"""

nums = sc.parallelize([1,5,3,9,4,0,2])
nums.count()

"""Collect"""

c = sc.parallelize(["Gnu", "Cat", "Rat", "Dog", "Gnu", "Rat"], 2)
c.collect()
c = sc.parallelize(["Gnu", "Cat", "Rat", "Dog", "Gnu", "Rat"], 2)
c.distinct().collect()

"""SaveasTextFile"""

a = sc.parallelize(range(1, 10000), 3)
a.saveAsTextFile("/usr/bin/mydata_a3")
x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,21], 3)
x.saveAsTextFile("/usr/bin/sample1.txt")

"""Foreach"""

def f(x): print(x)
sc.parallelize([1,2,3,4,5]).foreach(f)

"""Foreach Partition"""

def f(iterator):
  for x in iterator:
    print(x)
sc.parallelize([1,2,3,4,5]).foreachPartition(f)

"""Mathematical Actions"""

numbers = sc.parallelize(range(1,100))

numbers.sum()

numbers.min()

numbers.variance()

numbers.max()

numbers.mean()

numbers.stdev()

"""CountbyValue()"""

a = sc.parallelize([1,2,3,4,5,6,7,8,2,4,2,3,3,3,1,1,1])
a.countByValue()

"""toDebugString"""

a= sc.parallelize(range(1,19), 3)
b= sc.parallelize(range(1,13,3))
c= a.subtract(b)
c.toDebugString()

"""Creating Pair RDDs"""

rdd = sc.parallelize

"""Word Count Program"""

rdd = sc.textFile("Pyspark.txt")#creating an RDD

"""Filtering text data from an RDD"""

nonempty_lines = rdd.filter(lambda x: len(x) > 0)

"""Spliting words using flatMAp method"""

words = nonempty_lines.flatMap(lambda x: x.split(' '))

"""Counting the frequency of Each WOrd:"""

wordcount = words.map(lambda x:(x,1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[1], x[0])).sortByKey(False)

"""display"""

for word in wordcount.collect():
  print(word)

"""save file"""

wordcount.saveAsTextFile('content/wordcount')

"""passing function to Spark"""

rdd = sc.parallelize([1,2,3,4,5])
rdd.map(lambda x: x+2).collect()